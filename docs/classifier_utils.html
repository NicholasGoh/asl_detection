---

title: classfication_utils


keywords: fastai
sidebar: home_sidebar

summary: "utilities for training and visualizing/explaining classifier"
description: "utilities for training and visualizing/explaining classifier"
nb_path: "05_classifier_utils.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 05_classifier_utils.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="helper-class-to-plot-results-of-classification-in-real-time-taken-from-here">helper class to plot results of classification in real time taken from <a href="https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e">here</a><a class="anchor-link" href="#helper-class-to-plot-results-of-classification-in-real-time-taken-from-here"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PlotLosses" class="doc_header"><code>class</code> <code>PlotLosses</code><a href="https://github.com/NicholasGoh/asl_detection/tree/master/asl_detection/classification/utils.py#L8" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PlotLosses</code>() :: <code>Callback</code></p>
</blockquote>
<p>Abstract base class used to build new callbacks.</p>
<p>Attributes:
    params: dict. Training parameters
        (eg. verbosity, batch size, number of epochs...).
    model: instance of <code>keras.models.Model</code>.
        Reference of the model being trained.
    validation_data: Deprecated. Do not use.</p>
<p>The <code>logs</code> dictionary that callback methods
take as argument will contain keys for quantities relevant to
the current batch or epoch.</p>
<p>Currently, the <code>.fit()</code> method of the <code>Model</code> class
will include the following quantities in the <code>logs</code> that
it passes to its callbacks:</p>

<pre><code>on_epoch_end: logs include `acc` and `loss`, and
    optionally include `val_loss`
    (if validation is enabled in `fit`), and `val_acc`
    (if validation and accuracy monitoring are enabled).
on_batch_begin: logs include `size`,
    the number of samples in the current batch.
on_batch_end: logs include `loss`, and optionally `acc`
    (if accuracy monitoring is enabled).</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="helper-class-for-classifier-pipeline">helper class for classifier pipeline<a class="anchor-link" href="#helper-class-for-classifier-pipeline"> </a></h3><blockquote><p>pretrained Mobilenet is used to extract features from the dataset before training on these features to classify the 24 alphabets (<code>J</code> and <code>Z</code> requires motion)</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note code for visualization of feature maps is taken from <a href="https://github.com/gabrielpierobon/cnnshapes/blob/master/README.md">here</a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Classifier" class="doc_header"><code>class</code> <code>Classifier</code><a href="https://github.com/NicholasGoh/asl_detection/tree/master/asl_detection/classification/utils.py#L39" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Classifier</code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="GradCAM-used-to-explain-the-attention-of-the-model,-taken-from-here">GradCAM used to explain the attention of the model, taken from <a href="https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/">here</a><a class="anchor-link" href="#GradCAM-used-to-explain-the-attention-of-the-model,-taken-from-here"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GradCAM" class="doc_header"><code>class</code> <code>GradCAM</code><a href="https://github.com/NicholasGoh/asl_detection/tree/master/asl_detection/classification/utils.py#L334" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GradCAM</code>(<strong><code>model</code></strong>, <strong><code>classIdx</code></strong>, <strong><code>layerName</code></strong>=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

</div>
 

