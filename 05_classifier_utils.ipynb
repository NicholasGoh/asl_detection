{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp classification.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classfication_utils\n",
    "\n",
    "> utilities for training and visualizing/explaining classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper class to plot results of classification in real time taken from [here](https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import tensorflow as tf\n",
    "\n",
    "class PlotLosses(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper class for classifier pipeline\n",
    "> pretrained Mobilenet is used to extract features from the dataset before training on these features to classify the 24 alphabets (`J` and `Z` requires motion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note code for visualization of feature maps is taken from [here](https://github.com/gabrielpierobon/cnnshapes/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import cv2, os, telegram_send, tqdm, skimage, IPython, h5py, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "import asl_detection.save\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        self.categories = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "        self.category_map = {i: v for i, v in enumerate(self.categories)}\n",
    "        self.num_classes = len(self.categories)\n",
    "        self.img_size = 224\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.train_path, self.test_path = None, None\n",
    "        self.train_generator, self.test_generator = None, None\n",
    "        self.step_size_train, self.step_size_valid = None, None\n",
    "        self.images, self.labels = None, None\n",
    "        self.classifier = None\n",
    "        self.history = None\n",
    "        self.grad_cam_names = None\n",
    "        self.save_folder = None\n",
    "\n",
    "        self.feature_extractor = None\n",
    "        self.latent_vectors = None\n",
    "        self.latent_path = None\n",
    "        self.latent_test = None\n",
    "        self.latent_train = None\n",
    "\n",
    "\n",
    "    def generate_data(self, train_path, test_path, batch_size, figsize=(10,10), fontsize=16):\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        test_datagen = ImageDataGenerator(\n",
    "                rescale=1/255.)\n",
    "        train_datagen = ImageDataGenerator(\n",
    "                rescale=1/255.,\n",
    "                brightness_range=[.9, 1.],\n",
    "                rotation_range=5,\n",
    "                zoom_range=.1,\n",
    "                width_shift_range=.1,\n",
    "                height_shift_range=.1)\n",
    "        self.train_generator = train_datagen.flow_from_directory(\n",
    "                self.train_path,\n",
    "                shuffle=True,\n",
    "                target_size=(self.img_size, self.img_size),\n",
    "                color_mode='rgb',\n",
    "                batch_size=batch_size,\n",
    "                seed=0,\n",
    "                class_mode=\"categorical\")\n",
    "        self.test_generator = test_datagen.flow_from_directory(\n",
    "                self.test_path,\n",
    "                shuffle=True,\n",
    "                target_size=(self.img_size, self.img_size),\n",
    "                color_mode='rgb',\n",
    "                batch_size=batch_size,\n",
    "                seed=0,\n",
    "                class_mode=\"categorical\")\n",
    "\n",
    "        _, axes = plt.subplots(6, 6, figsize=figsize)\n",
    "        for i, category in enumerate(self.categories[:6]):\n",
    "            path = self.train_path + '/' + category\n",
    "            images = os.listdir(path)\n",
    "            for j in range(6):\n",
    "                image = cv2.imread(path + '/' + images[j])\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                axes[i, j].imshow(image)\n",
    "                axes[i, j].set(xticks=[], yticks=[])\n",
    "                axes[i, j].set_title(category, color = 'tomato').set_size(fontsize)\n",
    "        plt.suptitle('Vanilla Data').set_size(2*fontsize)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        images, labels = self.train_generator.next()\n",
    "        _, axes = plt.subplots(6, 6, figsize=figsize)\n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                image = images[i+j]\n",
    "                label = self.category_map[np.argmax(labels[i+j])]\n",
    "                axes[i, j].imshow(image)\n",
    "                axes[i, j].set(xticks=[], yticks=[])\n",
    "                axes[i, j].set_title(label, color = 'tomato').set_size(fontsize)\n",
    "        plt.suptitle('Augmented Data').set_size(2*fontsize)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        self.step_size_train=int(self.train_generator.n // self.train_generator.batch_size)\n",
    "        self.step_size_valid=int(self.test_generator.n // self.test_generator.batch_size)\n",
    "\n",
    "    def notify(self, fig):\n",
    "        fig.savefig('tmp.jpg')\n",
    "        with open('tmp.jpg', 'rb') as f:\n",
    "            telegram_send.send(images=[f])\n",
    "        os.remove('tmp.jpg')\n",
    "\n",
    "    def plot_accuracy(self, history):\n",
    "        f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        accuracy = history.history['accuracy']\n",
    "        loss = history.history['loss']\n",
    "        val_accuracy = history.history['val_accuracy']\n",
    "        val_loss = history.history['val_loss']\n",
    "        print('Training accuracy: {:.{}f}'.format(np.max(accuracy), 3))\n",
    "        print('Training loss: {:.{}f}'.format(np.max(loss), 3))\n",
    "        print('Validation accuracy: {:.{}f}'.format(np.max(val_accuracy), 3))\n",
    "        print('Validation loss: {:.{}f}'.format(np.max(val_loss), 3))\n",
    "        axes[0].plot(history.history['accuracy'])\n",
    "        axes[0].plot(history.history['val_accuracy'])\n",
    "        axes[0].set_title('Model accuracy')\n",
    "        axes[0].set(ylabel = 'accuracy', xlabel = 'Epoch')\n",
    "        axes[0].legend(['Train', 'Test'], loc='upper left')\n",
    "        axes[1].plot(history.history['loss'])\n",
    "        axes[1].plot(history.history['val_loss'])\n",
    "        axes[1].set_title('Model loss')\n",
    "        axes[1].set(ylabel = 'Loss', xlabel = 'Epoch')\n",
    "        axes[1].legend(['Train', 'Test'], loc='upper left')\n",
    "        return f\n",
    "\n",
    "    def set_feature_extractor(self, name = 'mobilenet', summary = False):\n",
    "        if name == 'mobilenet':\n",
    "            self.feature_extractor = MobileNet(input_shape = (self.img_size, self.img_size, 3), include_top=True,weights ='imagenet')\n",
    "            output = self.feature_extractor.layers[-6].output\n",
    "            self.feature_extractor = tf.keras.Model(self.feature_extractor.inputs, output)\n",
    "        if summary:\n",
    "            self.feature_extractor.summary()\n",
    "\n",
    "    def extract_and_save(self, latent_path, latent_vectors, save=True):\n",
    "        '''model: Model used to extract encoded features\n",
    "           generator: yields (x_batch, y_batch)\n",
    "        '''\n",
    "        self.latent_vectors = latent_vectors\n",
    "        self.latent_path = latent_path\n",
    "        if not save:\n",
    "            return\n",
    "        for folder in ['train', 'test']:\n",
    "            save_path = os.path.join(latent_path, folder)\n",
    "            if os.path.exists(save_path) and os.path.isdir(save_path):\n",
    "                shutil.rmtree(save_path)\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            template = 'batch_{}.h5'\n",
    "            batch = 0\n",
    "            for generator in [self.train_generator, self.test_generator]:\n",
    "                for x_batch, y_batch in tqdm.tqdm(generator):\n",
    "                    IPython.display.clear_output(wait=True)\n",
    "                    features = self.feature_extractor.predict(x_batch)\n",
    "                    file_path = os.path.join(save_path, template.format(batch))\n",
    "                    with h5py.File(file_path, 'w') as file:\n",
    "                        # encoded features and hard labels\n",
    "                        file.create_dataset('features', data=features)\n",
    "                        file.create_dataset('labels', data=y_batch)\n",
    "                    batch += 1\n",
    "                    if folder == 'train':\n",
    "                        if batch >= self.step_size_train:\n",
    "                            break\n",
    "                    else:\n",
    "                       if batch >= self.step_size_valid:\n",
    "                            break\n",
    "\n",
    "    def load_data(self, folder):\n",
    "        '''yields (x_batch, y_batch) for model.fit()\n",
    "        '''\n",
    "        root_path = os.path.join(self.latent_path, folder)\n",
    "        while True:\n",
    "            for file_path in os.listdir(root_path):\n",
    "                with h5py.File(os.path.join(root_path, file_path), 'r') as file:\n",
    "                    yield (np.array(file['features']), np.array(file['labels']))\n",
    "\n",
    "    def clear_session(self):\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    def train(self,\n",
    "              lr=None,\n",
    "              optimizer=None,\n",
    "              epochs=None,\n",
    "              decay_lr=False,\n",
    "              save_folder=None,\n",
    "              notification = False):\n",
    "\n",
    "        self.save_folder = save_folder\n",
    "\n",
    "        # shape of VGG16 encoded features\n",
    "        inputs = layers.Input(shape=self.latent_vectors)\n",
    "        x = layers.Dense(self.num_classes, activation='softmax')(inputs)\n",
    "        self.classifier = tf.keras.Model(inputs, x)\n",
    "\n",
    "        self.classifier.compile(optimizer=optimizer(lr=lr),\n",
    "                                loss='categorical_crossentropy',\n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "        def lr_decay(epoch):\n",
    "            alpha, decay = 1, 1\n",
    "            return lr / (alpha + decay * epoch)\n",
    "        callback_learning_rate = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=True)\n",
    "        plot_losses = PlotLosses()\n",
    "        callback_is_nan = tf.keras.callbacks.TerminateOnNaN()\n",
    "        callback_early = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta = .001, patience = 10)\n",
    "\n",
    "        callbacks = [plot_losses, callback_is_nan, callback_early]\n",
    "        callbacks += [callback_learning_rate] if decay_lr else []\n",
    "        self.latent_train, self.latent_test = 'train', 'test'\n",
    "\n",
    "        self.history = self.classifier.fit(\n",
    "                  x = self.load_data(self.latent_train),\n",
    "                  epochs=epochs,\n",
    "                  workers=15,\n",
    "                  steps_per_epoch=self.step_size_train,\n",
    "                  validation_steps=self.step_size_valid,\n",
    "                  validation_data=self.load_data(self.latent_test),\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "        fig = self.plot_accuracy(self.history)\n",
    "        if save_folder:\n",
    "            asl_detection.save.save(save_folder, 'acc_loss', fig=fig)\n",
    "            asl_detection.save.save(save_folder, 'model', self.classifier)\n",
    "        if notification:\n",
    "            self.notify(fig)\n",
    "        self.images, self.labels = self.test_generator.next()\n",
    "\n",
    "    def _visualize_feature_maps(self, image, _layers, scale):\n",
    "        model_layers = self.feature_extractor.layers\n",
    "        # Extracts the outputs\n",
    "        layer_outputs = [layer.output for layer in self.feature_extractor.layers]\n",
    "        # Creates a model that will return these outputs, given the model input\n",
    "        activation_model = tf.keras.Model(inputs=self.feature_extractor.inputs, outputs=layer_outputs)\n",
    "        # get activations\n",
    "        activations = activation_model.predict(image)\n",
    "        images_per_row = 4; count = -1\n",
    "        # Displays the feature maps\n",
    "        for layer, layer_activation in zip(model_layers, activations):\n",
    "            if not isinstance(layer, layers.Conv2D):\n",
    "                continue\n",
    "            count += 1\n",
    "            # show first 3 conv layers\n",
    "            if count != _layers:\n",
    "                continue\n",
    "            n_features = layer_activation.shape[-1] # Number of features in the feature map\n",
    "            size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
    "            n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
    "            display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "            for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
    "                for row in range(images_per_row):\n",
    "                    channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
    "                    # Post-processes the feature to make it visually palatable\n",
    "                    channel_image -= channel_image.mean()\n",
    "                    channel_image /= channel_image.std() + 1e-8\n",
    "                    channel_image *= 64\n",
    "                    channel_image += 128\n",
    "                    channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                    display_grid[col * size : (col + 1) * size, # Displays the grid\n",
    "                                 row * size : (row + 1) * size] = channel_image\n",
    "            fig_scale = scale / size\n",
    "            fig = plt.figure(figsize=(fig_scale * display_grid.shape[1],\n",
    "                                fig_scale * display_grid.shape[0]))\n",
    "            plt.title(layer.name)\n",
    "            plt.grid(False)\n",
    "            plt.imshow(display_grid, aspect='auto', cmap='gray')\n",
    "        if self.save_folder:\n",
    "            asl_detection.save.save(self.save_folder, 'feature_maps', fig=fig)\n",
    "\n",
    "    def visualize_feature_maps(self, index, _layers=1, scale=2):\n",
    "        image = self.images[index:index+1]\n",
    "        self._visualize_feature_maps(image, _layers, scale)\n",
    "\n",
    "    def generate_heat_map(self, _input):\n",
    "        self.grad_cam_names = [layer.name for layer in self.feature_extractor.layers if isinstance(layer, layers.Conv2D)]\n",
    "        image = self.images[_input:_input+1] if isinstance(_input, int) else _input\n",
    "        preds = self.classifier(self.feature_extractor(image))\n",
    "        idx = np.argmax(preds[0])\n",
    "        # initialize our gradient class activation map and build the heatmap\n",
    "        cam = GradCAM(self.feature_extractor, idx, self.grad_cam_names[-1])\n",
    "        heatmap = cam.compute_heatmap(image)\n",
    "        (heatmap, overlay) = cam.overlay_heatmap(heatmap, image, self.img_size, alpha=0.4)\n",
    "        label = self.category_map[idx]\n",
    "\n",
    "        if isinstance(_input, int):\n",
    "            description = 'image\\ntrue: {} pred: {}\\nconfidence: {:.3f}'.format\\\n",
    "            (self.category_map[np.argmax(self.labels[_input])], self.category_map[idx], preds[0][idx])\n",
    "        else:\n",
    "            description = 'pred: {}\\nconfidence: {:.3f}'.format(self.category_map[idx], preds[0][idx])\n",
    "\n",
    "        results = {'image': image, 'heatmap': heatmap, 'overlay': overlay, 'description': description, 'label': label}\n",
    "        return results\n",
    "\n",
    "    def visualize_heat_maps(self, index, rows=3, figsize=(8, 8)):\n",
    "        f, axes = plt.subplots(rows, 3, figsize=figsize)\n",
    "        for i in range(rows):\n",
    "            results = self.generate_heat_map(index+i)\n",
    "            \n",
    "            axes[i, 0].imshow(results['image'].reshape(self.img_size, self.img_size, 3))\n",
    "            axes[i, 1].imshow(results['heatmap'].reshape(self.img_size, self.img_size, 3))\n",
    "            axes[i, 2].imshow(results['overlay'].reshape(self.img_size, self.img_size, 3))\n",
    "            axes[i, 0].set_title(results['description']).set_size(12)\n",
    "            axes[i, 1].set_title('heatmap')\n",
    "            axes[i, 2].set_title('overlay')\n",
    "            axes[i, 0].axis('off')\n",
    "            axes[i, 1].axis('off')\n",
    "            axes[i, 2].axis('off')\n",
    "        plt.tight_layout(w_pad=0.1)\n",
    "        if self.save_folder:\n",
    "            asl_detection.save.save(self.save_folder, 'heat_maps', fig=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradCAM used to explain the attention of the model, taken from [here](https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GradCAM:\n",
    "    def __init__(self, model, classIdx, layerName=None):\n",
    "        # store the model, the class index used to measure the class\n",
    "        # activation map, and the layer to be used when visualizing\n",
    "        # the class activation map\n",
    "        self.model = model\n",
    "        self.classIdx = classIdx\n",
    "        self.layerName = layerName\n",
    "        # if the layer name is None, attempt to automatically find\n",
    "        # the target output layer\n",
    "        if self.layerName is None:\n",
    "            self.layerName = self.find_target_layer()\n",
    "    def find_target_layer(self):\n",
    "        # attempt to find the final convolutional layer in the network\n",
    "        # by looping over the layers of the network in reverse order\n",
    "        for layer in reversed(self.model.layers):\n",
    "            # check to see if the layer has a 4D output\n",
    "            if len(layer.output_shape) == 4:\n",
    "                return layer.name\n",
    "        # otherwise, we could not find a 4D layer so the GradCAM\n",
    "        # algorithm cannot be applied\n",
    "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
    "    def compute_heatmap(self, image, eps=1e-8):\n",
    "        # construct our gradient model by supplying (1) the inputs\n",
    "        # to our pre-trained model, (2) the output of the (presumably)\n",
    "        # final 4D layer in the network, and (3) the output of the\n",
    "        # softmax activations from the model\n",
    "        gradModel = tf.keras.Model(\n",
    "            inputs=[self.model.inputs[0]],\n",
    "            outputs=[self.model.get_layer(self.layerName).output,\n",
    "                self.model.output])\n",
    "        # record operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            # cast the image tensor to a float-32 data type, pass the\n",
    "            # image through the gradient model, and grab the loss\n",
    "            # associated with the specific class index\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOutputs, predictions) = gradModel(inputs)\n",
    "            loss = predictions[:, self.classIdx]\n",
    "        # use automatic differentiation to compute the gradients\n",
    "        grads = tape.gradient(loss, convOutputs)\n",
    "        # compute the guided gradients\n",
    "        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "        guidedGrads = castConvOutputs * castGrads * grads\n",
    "        # the convolution and guided gradients have a batch dimension\n",
    "        # (which we don't need) so let's grab the volume itself and\n",
    "        # discard the batch\n",
    "        convOutputs = convOutputs[0]\n",
    "        # guidedGrads = guidedGrads[0]\n",
    "        guidedGrads = grads[0]\n",
    "        # compute the average of the gradient values, and using them\n",
    "        # as weights, compute the ponderation of the filters with\n",
    "        # respect to the weights\n",
    "        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "        # grab the spatial dimensions of the input image and resize\n",
    "        # the output class activation map to match the input image\n",
    "        # dimensions\n",
    "        (w, h) = (image.shape[2], image.shape[1])\n",
    "        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "        # normalize the heatmap such that all values lie in the range\n",
    "        # [0, 1], scale the resulting values to the range [0, 255],\n",
    "        # and then convert to an unsigned 8-bit integer\n",
    "        numer = heatmap - np.min(heatmap)\n",
    "        denom = (heatmap.max() - heatmap.min()) + eps\n",
    "        heatmap = numer / denom\n",
    "        heatmap = ((1 - heatmap) * 255).astype(\"uint8\")\n",
    "        # return the resulting heatmap to the calling function\n",
    "        return heatmap\n",
    "    def overlay_heatmap(self, heatmap, image, img_size, alpha=0.2, colormap=cv2.COLORMAP_JET):\n",
    "        # apply the supplied color map to the heatmap and then\n",
    "        # overlay the heatmap on the input image\n",
    "        heatmap = cv2.applyColorMap(heatmap, colormap).reshape(1, img_size, img_size, 3)\n",
    "        output = image*255*(1-alpha) + heatmap.reshape(1, img_size, img_size, 3)*alpha\n",
    "        # return a 2-tuple of the color mapped heatmap and the output,\n",
    "        # overlaid image\n",
    "        output = np.uint8(output)\n",
    "        return (heatmap, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
